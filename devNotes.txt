
QUESTIONS:



---------------------------------------------------------------------------------------------------------

Lex test cases in ReadME. Add new parse test cases here during development

---------------------------------------------------------------------------------------------------------
March 13th

to do:
	-create helper funtions in parse.ts to enable matching, consuming, looking ahead, and possibly backtracking through tokens array
		** I can do this with a class based approach
	-remaining parse methods
	-output message on sucessful parse not just errors
	-show lexer output even if parse fails
	-display parse output on page (only in console at the moment)
	-make CST

---------------------------------------------------------------------------------------------------------
March 4th

to do:
	-Fix lexer
		-Completed (keywords inside strings should not throw errors)
		-tokenize funciton should consume one program at a time (determined by EOP) and produce a different [] for each

	-Parsing to do
		-write derivation rules
			-as nested functions
		-loop through them against tokens[] and backtrack appropriately to match token stream
		-consume tokens when they match derivation
		-print output to user
---------------------------------------------------------------------------------------------------------
Feb 17th

to do:
	-verify order or toxenRegex array so longest match is enforced
	-remove /b word boundary so lex can uccur without spaes properly
	-add some boolFlags like inBrace, inQuotes, inBlock(?)
	-implement warning handling

Strings are not lexemmes so I should remove it from the regex, instead detect quotes and identify each piece as a char or space then anything else until next quote as invalid symbols

Lets treat comments like strings in that we should recognize the start and end seperately and anything in between will be transmuted to a comment
we can even skip looping through regex def while inComment is true

anything inside a comment doesn't get added to tokens[] so I can;t look at the last token and check inComment as a way to check for unterminated comments. We'll need to handle this flag some other way. What if I looked for com_start and see if I ever get com_end?

Wait that still doesnt work since com start and end are not added to tokens[]

Ok but it does work for quotes

I realized I can add the inQuote, inComment, and missingEOP flags to the return of the tokenize function. That way I can avoid logic in index.ts  and eliminate redundancy

The lexer works as expected now. We just need to handle multiple programs in one input, this will likely require a helper function to split the logic and maybe another tokens[] array	
---------------------------------------------------------------------------------------------------------

Feb 12th
2pm

The site is live, lexer is running on input code and displaying an output. Enums have been properly adjusted but there still may be some work to do. Currently we are not handling mutliple programs properly and there is no logic at all to handle warnings, only errors.
Goal is to continue testing and adjusting the logic in index.ts formatTokens() to handle this.
After that we'll make sure that all of Alan's test cases return the expected output. Then we can use crazier test cases and continue imporving code
	todo:
		handle mulitple programs in one input
		recognize warnings
		add $ (EOP) if none is present
		change text appearance for warnings and errors
---------------------------------------------------------------------------------------------------------

Feb 11

previous work was mostly wrong but rereading the chapter and instructions I understand better

**New Branch** - newStruct

Structure of project changes to use browser and remove node-modules from git

lexer directory removed and lexer.ts and index.ts places in source folder

Now I'll move the enum I created earlier (and backed up locally) to lexer.ts

lexer.ts should also do the following:

	define token structure (type, value, position (line and column?))
	distinguish between tokens with RegEx formulas
	take and input (test code) and 'tokenize' what it gets. ie use the structure and regex to assign what it reads as valid or invalid tokens
	find errors and warnings
	log the output accordingly

index.ts

	create JS for index.html.

---------------------------------------------------------------------------------------------------------
Feb 2

Commit 0:
project structure intailaized
there might be issues with where JS code generates
how to implement dist folder?

Commit 1: 
file reader method set up and log message works properly

Commit 2:
token list created as enum in src/lexer/token.ts
is this token list complete? 
Do I need to add digits and charecters?

Goal:
Check for valid keywords, identifiers, symbols, digits, charecters

steps
Charecter has
	-position (Line and number)
	-content
	-parents
	-expected partner (open brace expects closed brace, quotations, etc)
	
Token has
	-start position
	-end position
	-content
	
	
We need to read charecters, sorth them into tokens, and check their validity

test cases will be in local folder but can be any name or extension
---------------------------------------------------------------------------------------------------------
