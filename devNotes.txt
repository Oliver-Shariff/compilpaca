---------------------------------------------------------------------------------------------------------
May 7th

	-Addition with variables works in base and chained cases
	-NotEquals loop works after implementing jump table
	-Let's extend the logic here to while Equals and the if statement
	-First we'll test the current while notEquals with different types of variables
	-ok I just need to add support for booleans
---------------------------------------------------------------------------------------------------------
May 5th: 

if values are equal z flag = 1
	-Issue, string values are overwritted to their final value since they are written into the heap so code like
	{    string c
		c = "hello"
		string d 
		d = c
		print(c)
		print(d)
	
		d = "alan"
		print(d)
		}$

	prints helloalanalan instead of hellohelloalan

	-I'm realizing that my generate expression function doesn't work
	Todo:
	DONE -fix interger addition
	DONE - fix string assignment through id ex
			string a a = "hello" string b b = a
		-handle boolean intialization and assignment
		-if statements
		-while statements


---------------------------------------------------------------------------------------------------------
May 2nd

	-Assignment, expressionsm strings and heap memory handled
	- Let's work on print statements so I can test these properly
	-

---------------------------------------------------------------------------------------------------------
May 1st - Code gen

	-Alright scope is working now, let's work on single scope code gen first
	-For var decls I have int string and bool
	-for now I'll handle int and bool the same way for intialization
	other cases:
		-Assignments
		-Expressions
		-Print
		-If / While
		-String handling
		-Final memory (fillStatic)
	

---------------------------------------------------------------------------------------------------------
May 1st - Scope fixes

	-OK we can decorate the AST to handle code gen but first I need to fix the scope/type issues
	-All the issues boil down to type enforecement
	-I'm pulling out the node for the "value" in an assignment statement
	-It doesn't work for addition because of nesting but I can fix that later with some kind of helper function
	-I need some logic that infers the type of the value
	-hmm I realized that assigning an empty string breaks this too
		-ok I fixed that
	-alright type enforcement is handled now, I had to write a helper function that checks the inherent type
	-this function also needs to use recursion for nested assignment statements like boolexprs and intexprs
	-Lets change scopeLevel to scopeId - that way parallel scopes are not confused
---------------------------------------------------------------------------------------------------------
April 28th

	-added value to static Obkect
	-gnerating code at this point require AST and scope tree traversal, We could geneate code at the same time as semantic analysis but that is not a modular approach
	-There mus tbe an easier way to check scope while traversing the AST
	-Can semanticAnalyzer add a scope level/id to the TreeNode while building the scope tree?
	-this way code gen can just take in the AST

---------------------------------------------------------------------------------------------------------
April 25th

	-Let's backtrack now, after some thought I think the alternate approach to the static table will work better, where the static object itself knows all the positions it belongs
	-let's also finalize everything the static table needs to do accounting for multi scope and loops
	static object:
		-variable's name
		-scope
			-these two form the primary key for the variable
		-length (how many spots does it need in memory)
		-locations[] (this should be an array or list)
	-The static table will be an array of these objects
	-ok now let's build out the static class and some helper functions
	-When we come across a variable we need to see if it exists in the table before entering it
	-the primary key for a variable is its name and scope
	-To check scope I'll need a way to check my postion in the scope tree as I traverse the AST
	-



---------------------------------------------------------------------------------------------------------
April 22nd:

	-I got the intial case switch working, we're parsing the AST to add op codes and operands into the code array
	-we need to handle the static table now
		-create a static object that holds
			-temp address T0, T1, T2 etc
			-id name (not using var since it is a keyword in TS)
			-actual address or address length

		-On entry creation I need to:
			-create this static object
			-put in in the static table

		-Considerations:
			-check if Id already exists
			-how will I back fill in the code array?
				-The static object either needs to hold an array with all the locations it exists in the code array
				OR
				-I need to loop through the code array for find and replace
				^ that's easier
				ok so I'll change address to a string, store the address as the intial operand and the following can always be 00 since we only have a 256byte array
				-wait but can I easily increment a string? and what if there are more than 10 variables? once I pass t9 the find and replace will break
				-with the grammar we can have 26 unique variables, let's account for that
				-I could use capital letters A-Z, that will allow incrementing and won't get confused with var names since those can only be lowercase
				-I may need to adjust the code array since I think it can only hold number right now

		ok so I come across and id
			-check if it exists in the staic table with node.name in static[]
				-if it does use the existing temp name
				-if it doesn't
					-prevTemp = staticTable[staticTable.length-1].temp
					-prevAdd = staticTable[staticTable.length-1].address
					-new temp = prevTemp++
					-newAdd = ...

				-hmm address is more like offset
				-how is it ACTUALLY used?

		-side note with additional scope we can have more than 26 unique variables
			-two letters gives us 26^2 so 676 
		-I should probably use lowercase so it doesn't get confused with opcodes




---------------------------------------------------------------------------------------------------------
April 21st:

	-Had to fix isUsed and isIntialized in the scopeAnalyzer
	-We can now move onto code gen if there are no errors with sematic analysis
	-code gen should only need the AST and symbol table and only return an array or an error
	-that logic is now handled in index.ts
	- I need to remember to moves all lexing logic to lexer.ts from index.ts later
	-first lets get this array printing nicely so I can debug as I go

	-OK I had to adjust scopeAnalyzer again to return the scope tree so codeGen can see it
	-The array is filled with 0x00 and output is displayed cleanly

	-I've initialized a static table, later I'll need a jump table when working on conditional statements
	-now let's work on traversing the AST and using case switch statements to fill the code array and static table
	-I'm wondering if I even needed the scope tree...


---------------------------------------------------------------------------------------------------------

Lex test cases in ReadME. Add new parse test cases here during development
---------------------------------------------------------------------------------------------------------
March 17th: 

	-CST displaying properly
	-consistent way of adding nodes and ending children implemented throughout parsing methods
	-user functionality works, don't parse if lex fails, no CST if parse fails.
		-need to add EOP if missing from lex
	
	To DO: 
	-parse 
		X-intExpr
		X-intop
		X-id
		X-type
		X-char
		X-space ? //handled in charlist
		X-digit
		X-boolop
		X-boolval
	
		
---------------------------------------------------------------------------------------------------------
March 13th

to do:
	-create helper funtions in parse.ts to enable matching, consuming, looking ahead, and possibly backtracking through tokens array
		** I can do this with a class based approach
	-remaining parse methods
	-output message on sucessful parse not just errors
	-show lexer output even if parse fails
	-make CST
	-Comments breaker parser
	
	-display parse output on page (only in console at the moment)
		instead of throwing errors lets creates a message and a log function that can be returned with the parser call

	-messages are being displayed to user properly
	-ok I've been combing the output of the CST parse
	-lets seperate these clearly. I don't want to show consumed tokens in the parse, only the tree
	
	
---------------------------------------------------------------------------------------------------------
March 4th

to do:
	-Fix lexer
		-Completed (keywords inside strings should not throw errors)
		-tokenize funciton should consume one program at a time (determined by EOP) and produce a different [] for each

	-Parsing to do
		-write derivation rules
			-as nested functions
		-loop through them against tokens[] and backtrack appropriately to match token stream
		-consume tokens when they match derivation
		-print output to user
---------------------------------------------------------------------------------------------------------
Feb 17th

to do:
	-verify order or toxenRegex array so longest match is enforced
	-remove /b word boundary so lex can uccur without spaes properly
	-add some boolFlags like inBrace, inQuotes, inBlock(?)
	-implement warning handling

Strings are not lexemmes so I should remove it from the regex, instead detect quotes and identify each piece as a char or space then anything else until next quote as invalid symbols

Lets treat comments like strings in that we should recognize the start and end seperately and anything in between will be transmuted to a comment
we can even skip looping through regex def while inComment is true

anything inside a comment doesn't get added to tokens[] so I can;t look at the last token and check inComment as a way to check for unterminated comments. We'll need to handle this flag some other way. What if I looked for com_start and see if I ever get com_end?

Wait that still doesnt work since com start and end are not added to tokens[]

Ok but it does work for quotes

I realized I can add the inQuote, inComment, and missingEOP flags to the return of the tokenize function. That way I can avoid logic in index.ts  and eliminate redundancy

The lexer works as expected now. We just need to handle multiple programs in one input, this will likely require a helper function to split the logic and maybe another tokens[] array	
---------------------------------------------------------------------------------------------------------

Feb 12th
2pm

The site is live, lexer is running on input code and displaying an output. Enums have been properly adjusted but there still may be some work to do. Currently we are not handling mutliple programs properly and there is no logic at all to handle warnings, only errors.
Goal is to continue testing and adjusting the logic in index.ts formatTokens() to handle this.
After that we'll make sure that all of Alan's test cases return the expected output. Then we can use crazier test cases and continue imporving code
	todo:
		handle mulitple programs in one input
		recognize warnings
		add $ (EOP) if none is present
		change text appearance for warnings and errors
---------------------------------------------------------------------------------------------------------

Feb 11

previous work was mostly wrong but rereading the chapter and instructions I understand better

**New Branch** - newStruct

Structure of project changes to use browser and remove node-modules from git

lexer directory removed and lexer.ts and index.ts places in source folder

Now I'll move the enum I created earlier (and backed up locally) to lexer.ts

lexer.ts should also do the following:

	define token structure (type, value, position (line and column?))
	distinguish between tokens with RegEx formulas
	take and input (test code) and 'tokenize' what it gets. ie use the structure and regex to assign what it reads as valid or invalid tokens
	find errors and warnings
	log the output accordingly

index.ts

	create JS for index.html.

---------------------------------------------------------------------------------------------------------
Feb 2

Commit 0:
project structure intailaized
there might be issues with where JS code generates
how to implement dist folder?

Commit 1: 
file reader method set up and log message works properly

Commit 2:
token list created as enum in src/lexer/token.ts
is this token list complete? 
Do I need to add digits and charecters?

Goal:
Check for valid keywords, identifiers, symbols, digits, charecters

steps
Charecter has
	-position (Line and number)
	-content
	-parents
	-expected partner (open brace expects closed brace, quotations, etc)
	
Token has
	-start position
	-end position
	-content
	
	
We need to read charecters, sorth them into tokens, and check their validity

test cases will be in local folder but can be any name or extension
---------------------------------------------------------------------------------------------------------
